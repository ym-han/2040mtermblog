<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Post 2 of 3 | λ</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Post 2 of 3" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Team λ’s 2nd midterm project blog post" />
<meta property="og:description" content="Team λ’s 2nd midterm project blog post" />
<link rel="canonical" href="https://ym-han.github.io/2040mtermblog/markdown/2020/03/15/post2.html" />
<meta property="og:url" content="https://ym-han.github.io/2040mtermblog/markdown/2020/03/15/post2.html" />
<meta property="og:site_name" content="λ" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-15T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://ym-han.github.io/2040mtermblog/markdown/2020/03/15/post2.html","@type":"BlogPosting","headline":"Post 2 of 3","dateModified":"2020-03-15T00:00:00-05:00","datePublished":"2020-03-15T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://ym-han.github.io/2040mtermblog/markdown/2020/03/15/post2.html"},"description":"Team λ’s 2nd midterm project blog post","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/2040mtermblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ym-han.github.io/2040mtermblog/feed.xml" title="λ" /><link rel="shortcut icon" type="image/x-icon" href="/2040mtermblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/2040mtermblog/">λ</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/2040mtermblog/about/">About Us</a><a class="page-link" href="/2040mtermblog/search/">Search</a><a class="page-link" href="/2040mtermblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Post 2 of 3</h1><p class="page-description">Team λ's 2nd midterm project blog post</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-03-15T00:00:00-05:00" itemprop="datePublished">
        Mar 15, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/2040mtermblog/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#the-resnet-model-a-tweaked-version-of-our-second-baseline-model">The ResNET model (a tweaked version of our second baseline model)</a>
<ul>
<li class="toc-entry toc-h2"><a href="#what-is-the-resnet-architecture">What is the ResNET architecture</a></li>
<li class="toc-entry toc-h2"><a href="#what-we-did-this-time">What we did this time</a></li>
<li class="toc-entry toc-h2"><a href="#links--resources--references">Links / resources / references</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#efficientnet-model">EfficientNet model</a>
<ul>
<li class="toc-entry toc-h2"><a href="#efficientnet-architecture">EfficientNet architecture</a></li>
<li class="toc-entry toc-h2"><a href="#what-we-did">What we did</a></li>
<li class="toc-entry toc-h2"><a href="#resources--references">Resources / references</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#inception">Inception</a>
<ul>
<li class="toc-entry toc-h2"><a href="#inception-v3-architecture">Inception v3 Architecture</a></li>
<li class="toc-entry toc-h2"><a href="#resources--references-1">Resources / references</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li>
</ul><p>This is the second of the blog posts for our midterm project in Data 2040, a deep learning class at Brown University. As quick reminder, the first blog post <a href="https://emmanuel-peters.medium.com/blog-post-1-cassava-leaf-distribution-c6c3232be9d5">can be accessed here.</a></p>

<p>We explored 3 architectures for this prediction task: ResNET, InceptionNet, and EfficientNet. As we’ll see, the EfficientNet model ended up having the highest accuracy.</p>

<h1 id="the-resnet-model-a-tweaked-version-of-our-second-baseline-model">
<a class="anchor" href="#the-resnet-model-a-tweaked-version-of-our-second-baseline-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>The ResNET model (a tweaked version of our second baseline model)</h1>

<p>One simple approach we took to build upon our second, and more sophisticated, baseline model; this was a (slightly) finetuned version of a ResNET50 model, <a href="https://www.kaggle.com/muellerzr/cassava-fastai-starter">courtesy of Zach Mueller</a>.</p>

<p>Since we’ve already explained the rough contours of the model in our previous blog post, we won’t spend much time here talking about we did the previous round. That said, it’s worth briefly explaining the ResNET architecture.</p>

<h2 id="what-is-the-resnet-architecture">
<a class="anchor" href="#what-is-the-resnet-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is the ResNET architecture</h2>

<p>This was invented by Kaiming He et al in 2015; the key innovation, at the risk of over-simplifying, was to use so-called ‘skip connections’ — connections between layers that skip over layers — that make deeper networks easier to train. (For more details, see either <a href="https://arxiv.org/abs/1512.03385">the original paper</a>, or <a href="https://github.com/fastai/fastbook%20and%20http://d2l.ai/">FastAI’s explanations of it</a>.</p>

<h2 id="what-we-did-this-time">
<a class="anchor" href="#what-we-did-this-time" aria-hidden="true"><span class="octicon octicon-link"></span></a>What we did this time</h2>

<p>As we’d mentioned in the previous blogpost, the model was implemented using the FastAI library. Although our previous version of the model employed some data augmentation, we’d confined ourselves to the basic ones that are already built into FastAI; our main experiment this time was to try doing even more data augmentation with albumentations library (<a href="https://www.kaggle.com/muellerzr/recreating-abhishek-s-tez-with-fastai">by adapting another one of Mueller’s notebooks</a>). In particular, we added transforms like hue and saturation tweaks, as well as <a href="https://arxiv.org/pdf/1708.04552.pdf">‘cutout’</a> and ‘coarse dropout’. The last two are worth emphasizing: they basically involve randomly remove rectangles from the image during training (cutout removes one large but otherwise randomly sized square, whereas coarse dropout removes many small similarly-sized rectangles).</p>

<p>Adding these augmentations turned out to be more effective than we’d had expected. This model had already got to <strong>79%</strong> validation accuracy with just 1 frozen epoch and 3 epochs, even before adding these more sophisticated augmentations; after adding them, we got to <strong>84%</strong> accuracy (with the same number of epochs). That’s a big difference, for not very much work.</p>

<p><img src="/2040mtermblog/images/second_post/loss_accs_firstmodel.png" alt="" title="Losses and validation accuracies for the model, before and after adding these augmentations">
<img src="/2040mtermblog/images/second_post/resnet_loss_acc_curve_aug_model.png" alt="" title="Losses and validation curves for the model, after adding these augmentations"></p>

<p>But why, you might ask, did adding these augmentations make such a big difference? We speculate that it’s because these augmentations — as with any sort of data augmentation — serve to regularize the model, preventing it from overfitting to the training data. (This, of course, doesn’t yet answer why it made such a <em>big</em> difference. But trying to answer that would bring us too far afield.)</p>

<h2 id="links--resources--references">
<a class="anchor" href="#links--resources--references" aria-hidden="true"><span class="octicon octicon-link"></span></a>Links / resources / references</h2>

<ul>
  <li>Our Colab notebook for both the baseline and tweaked model <a href="https://colab.research.google.com/drive/1n_jN7X1D8kB9lCicYcYaFvzwUCVViXg5?usp=sharing">can be found here</a>.</li>
  <li>Again, in making this notebook, we adapted two of Zach Mueller’s notebooks: https://www.kaggle.com/muellerzr/cassava-fastai-starter and https://www.kaggle.com/muellerzr/recreating-abhishek-s-tez-with-fastai</li>
  <li><a href="https://arxiv.org/abs/1512.03385">Here’s the paper that introduced ResNET to the world.</a></li>
  <li>And the paper on cutout <a href="https://arxiv.org/pdf/1708.04552.pdf">can be found here</a>
</li>
</ul>

<h1 id="efficientnet-model">
<a class="anchor" href="#efficientnet-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>EfficientNet model</h1>

<p>The second architecture we tried was EfficientNet, <a href="https://www.kaggle.com/bununtadiresmenmor/starter-keras-efficientnet">via this notebook</a>.</p>

<h2 id="efficientnet-architecture">
<a class="anchor" href="#efficientnet-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>EfficientNet architecture</h2>

<p>EfficientNet uses the compound-scaling(balancing dimensions of the width, depth, and resolution by scaling up them with a constant ratio) method to heuristic scale-up convolution neural networks <a href="https://arxiv.org/pdf/1905.11946.pdf">(Tan and Le, 2019)</a> to avoid computational expensive grid-search of hyperparameter. There is a family of EfficientNet models that makes up a good combination of efficiency and accuracy on various scales. Keras API provides pre-trained EfficientNet models with different variants from B0 to B7. The input shape is different for each of these variants.</p>

<h2 id="what-we-did">
<a class="anchor" href="#what-we-did" aria-hidden="true"><span class="octicon octicon-link"></span></a>What we did</h2>

<p>We implemented the pre-trained EfficientNetB3 model based on the input image resolution. On top of the pre-trained EfficientNetB3 model, we first did global average pooling. Then we added a dense layer with 256 neurons, with an activation function of Relu, followed by a dropout layer with a dropout rate of 0.5. Finally a dense output layer with softmax activation function with 5 neurons. The model architecture looks like this:</p>

<p><img src="/2040mtermblog/images/second_post/effNetArch.png" alt="" title="EfficientNet architecture"></p>

<p>We implemented Early Stopping and ReduceLROnPlateau callbacks in the model. The early stopping callback stop training once a monitored metric has stopped improving. In our case, the monitored metric is validation loss. The model training loop will check at the end of every epoch if the validation loss is no longer decreasing based on some user-customized criteria. For example, in our model, we set min_delta = 0.001, which is the minimum change in the validation loss we set to qualify as an improvement, and patience = 7, the number of epochs with no improvement after which training will be stopped. Once the model training loop found the validation loss is no longer decreasing based upon these criteria, the training will terminate. Similarly, the ReduceLROnPlateau callback reduces the learning rate when the monitored metric stopped improving. Many models would benefit from reducing the learning rate by a factor of 2-10 once learning stopped. It allows us to monitor the metric we are interested in; and if we see no improvement for patience (we set to 2)  number of epochs, the learning rate will be reduced.</p>

<p>We used the Adam optimizer, the categorical cross-entropy with 0.3 label smoothing was used given there is some mislabelling in the dataset. We split the data 80% for training, 20% for testing.  We set to train the model for 30 epochs but it stopped early at 24 epochs since the early stopping criteria has been met. Finally, the model achieved an accuracy of around <strong>89%</strong>.</p>

<p>Train and validation loss and accuracy figures are shown below:</p>

<p><img src="/2040mtermblog/images/second_post/lossValEffCurves.png" alt="" title="Train val loss accuracy curves"></p>

<h2 id="resources--references">
<a class="anchor" href="#resources--references" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources / references</h2>

<p>Tan and Le,2019. https://arxiv.org/pdf/1905.11946.pdf</p>

<p><a href="https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning">Keras’ tutorial on fine-tuning Efficient Net</a></p>

<h1 id="inception">
<a class="anchor" href="#inception" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inception</h1>

<p>The third model we tried is inception-v3 pre-trained on ImageNet. Compared to EfficientNet, it performs slightly worse on ImageNet and on the Cassava dataset. It gave a validation/testing accuracy of 84%. Inception-v3 is the third version of Google’s inception network series (googlenet/inception) and is proven to be significantly computationally efficient.</p>

<h2 id="inception-v3-architecture">
<a class="anchor" href="#inception-v3-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inception v3 Architecture</h2>

<p>The architecture of an Inception v3 network is progressively built, step-by-step, as follows:</p>

<ol>
  <li>Factorized convolutions to reduce parameters</li>
  <li>Smaller convolutions</li>
  <li>Asymmetric convolution</li>
  <li>Auxiliary classifier: an auxiliary classifier is a small CNN inserted between layers during training, and the loss incurred is added to the main network loss</li>
  <li>Grid size reduction</li>
</ol>

<p>For the head of the model we added a dropout layer and a dense softmax layer with 5 neurons. The optimizer used was SGD and loss was CategoricalCrossEntropy with 0.2 label smoothing. The label smoothing is because there is some amount of mislabelling in the dataset. The base notebook we used also used Xception for the base model, however since there were very small differences in the validation accuracy and loss, we chose to keep Inception-v3 as our 3rd model.</p>

<p>We tried training with multiple epochs but there was no significant increase in val. accuracy after 8 epochs (as in the original notebook) and since every epoch took upwards of 10 minutes for us, in the interest of time we kept it at 8. The validation accuracy we got in the end was <strong>84%</strong>.</p>

<h2 id="resources--references-1">
<a class="anchor" href="#resources--references-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources / references</h2>

<p><a href="https://arxiv.org/pdf/1409.4842.pdf">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., &amp; Rabinovich, A. (2014). Going deeper with convolutions.</a></p>

<p><a href="https://blog.paperspace.com/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/">Vihar Kurama. (2020, June 5). A Guide to ResNet, Inception v3, and SqueezeNet 
Paperspace Blog. Paperspace Blog; Paperspace Blog.</a></p>

<p><a href="https://www.kaggle.com/junyingsg/end-to-end-cassava-disease-classification-in-keras">Jun Ying’s Kaggle notebook</a></p>

<p><a href="http://stanford.edu/class/ee367/Winter2019/bergman_report.pdf">Bergman A, Lindell D. Factorized Convolution Kernels in Image Processing. Accessed March 16, 2021.</a></p>

<h1 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h1>

<p>We’ve seen how EfficientNet got the best accuracy of the three architectures. Although this obviously wasn’t a carefully controlled comparison — there were a thousand and one variables that we didn’t control — it’s at least suggestive, especially in light of how the best-performing teams on this Kaggle competition also used EfficientNet.</p>

<p>What further steps will we take? Perhaps the most obvious, in light of our discussion, is: use EffficientNet! Less obviously, but more importantly, since at least some of our experiments were performed on <em>im</em>balanced validation data sets, we also want to try seeing how our models perform with more balanced ones (c.f. our teacher Bo Qing’s comments on another group’s work during office hours). We also hope to carry out more experiments with data augmentation, and of course do the almost-obligatory hyperparameter tuning. Finally, we have a bunch of snazzy model interpretability visualizations lined up, so stay tuned!</p>

  </div><a class="u-url" href="/2040mtermblog/markdown/2020/03/15/post2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/2040mtermblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/2040mtermblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/2040mtermblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Midterm Project Blog for Data2040</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
