{
  
    
        "post0": {
            "title": "Cassava leaf disease classification ensemble models (Post 3 of 3)",
            "content": "Cassava leaf disease classification ensemble models . By Peipei Li, Emmanuel Peters, Yongming Han . This is the final blog post of our data2040 midterm group project on cassava leaf disease classification. . Here are the links to the previous posts: . Initial blog post https://emmanuel-peters.medium.com/blog-post-1-cassava-leaf-distribution-c6c3232be9d5 . Midway blog post https://ym-han.github.io/2040mtermblog/markdown/2020/03/15/post2.html . We previously tried three different models: ResNet, EfficientNet, and InceptionNet. In the final model, we decided to try to ensemble those three models in a variety of ways to see if we could make better predictions of the leaf disease classification. . Now, given how imbalanced the data was, a natural question is: how did we construct the validation set? In particular, what was the distribution of data sets that we used to train and validate our models? . One way to do this, of course, is to manually construct a more balanced dataset. We didn’t end up doing this, though it’s at the top of the list of Things We Would Do Had We More Time. Instead, we simply used the distribution of the test set on Kaggle as the distribution of our validation set (this turned out to be more or less what you’d get with by randomly sampling the data, so it was no work at all). . . . How did we get the distribution of the test set? We did by submitting a constant classifier for each of the classes. That is, for each class, we submitted a notebook that predicted that class on all of the inputs. The accuracy of that classifier will in effect be the prevalence of the class in the test data. And since Kaggle helpfully tells us how accurate submitted notebooks are, we can recover the distribution of the test data by submitting a constant classifier for each of the five classes. . “In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.” . https://en.wikipedia.org/wiki/Ensemble_learning . For our final model we tried various approaches to ensemble the models we trained for our Midway blog post. . Approach 1 . . We started by doing a simple average of our three models (ResNet50, Inception, EfficientNet) we talked about during our midterm blogpost, this gave us an accuracy score of 80%. This rather low score is probably because our first model (ResNet50) had not been tuned properly. To combat this, there were two approaches: . Tune the ResNet50 and retrain it. . | Use a weighted average with a lower weight on ResNet50 We decided on starting with 2. so we looped over a grid of weights, and improved performance on the validation set to ~86%. . | The second approach we did was to try ensembling different combination of two out of three of our pre-trained models. Specifically, we tried the combination of ResNet and InceptionNet, EfficientNet and InceptionNet, and ResNet and EfficientNet. For this, all we had to change was the grid to include 0 and only print cases where two networks had non zero weights. It should be noted that on certain samplings of the validation set we had single networks performing better than any combination including them, but on average we got higher performance for combinations of two or more. . We got the highest weighted average of combination for InceptionNet and EfficientNet for an accuracy of 85.6/86%. . Approach 2 . . Instead of using a simple average on the weights outputted by the softmax of the different CNNs, the second approach was to use stacking. To do this we needed to make sure the input tensors to the CNNs were similar and thus we had to retrain the models and make sure they fit a singular workflow. So, we focussed on a better tuned ResNet50 and Inception (we made this choice despite the higher accuracy score of EfficientNet because we had to retune ResNet50 anyway and Inception was the most computationally efficient network out of the 3). . For stacking, we take the outputs of two different networks and use them as an input to an “ensemble” network. . It can be argued that all ensembling is a form of stacking. The highest accuracy score we got for this was also ~84%. . Approach 3 . While looking for documentation for InceptionNet we came across this blogpost https://ai.googleblog.com/2016/08/improving-inception-and-image.html. Which talks about a stack of InceptionNetV3 with various Residual blocks from ResNet. We used this as our third ensemble. It trained faster than Approach 2, and gave us higher validation accuracy on most random seeds and validation subsets. Stacking resnet and inception (done by google)- ResnetInceptionV2. ~85% . Some plots / figures for InceptionResnetv2 . . Classification Report for InceptionResNet precision recall f1-score support 0 0.48 0.34 0.40 200 1 0.73 0.75 0.74 436 2 0.78 0.66 0.72 495 3 0.95 0.94 0.95 2638 4 0.59 0.74 0.66 510 accuracy 0.84 4279 macro avg 0.71 0.69 0.69 4279 weighted avg 0.84 0.84 0.84 4279 . Confusion matrix for InceptionResNet . . As we can see from the confuson matrix, class 3 (Cassava Mosaic Disease (CMD)) has the highest chance of been correctly classified. This is not surprising because the CMD leaf has severe shape distortion and mosaic patterns. . By contrast, our model has the most difficult time with class 0 (Cassava Bacterial Blight (CBB)). Although the Cassava Bacterial Blight type has some been documented as angular spots, brown spots with yellow borders, yellow leaves, leaves wilting, those characteristics are not prominent or salient enough as features in the rest of classes. Of course, the way that the dataset was imbalanced partially contributed to the results. . Some pictures of the two classes mentioned above are shown here: . . . Approach 4 . We finally come to our best model where we used a weighted average on the Approach 3 stack with EfficientNetB3. . This gave us the highest validation accuracy at ~87% with our pretrained models. So we decided to spend some time changing the hyperparameters and improving the individual CNNs. Unfortunately, in the interest of time, we had to keep the number of epochs low and thus never hit our early stopping callbacks. This was true even when patience value was set to &lt;5. We also added label smoothing to InceptionResNetV2 and reduced level of label smoothing in EfficientNet. . Our final Validation accuracy score was ~89%. . Resources/Citations . Improving Inception and Image Classification in TensorFlow. (2016, August 31). https://ai.googleblog.com/2016/08/improving-inception-and-image.html | Szegedy, C., Ioffe, S., Vanhoucke, V., &amp; Alemi, A. (2017). Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 31(1). Retrieved from https://ojs.aaai.org/index.php/AAAI/article/view/11231 | https://www.kaggle.com/junyingsg/end-to-end-cassava-disease-classification-in-keras | https://www.kaggle.com/iamyajat/cassava-leaf-disease-inceptionresnetv2 | https://www.kaggle.com/danpotter/blind-monkey-submission-example-data2040-sp21/data?select=submission.csv | Maxim Mikhaylov. (2017, December 13). Ensembling ConvNets using Keras https://towardsdatascience.com/ensembling-convnets-using-keras-237d429157eb | Adrian Rosebrock Deep Learning for Computer Vision with Python – Practitioner Bundle | .",
            "url": "https://ym-han.github.io/2040mtermblog/markdown/2020/03/21/post3.html",
            "relUrl": "/markdown/2020/03/21/post3.html",
            "date": " • Mar 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Post 2 of 3",
            "content": "This is the second of the blog posts for our midterm project in Data 2040, a deep learning class at Brown University. As quick reminder, the first blog post can be accessed here. . We explored 3 architectures for this prediction task: ResNET, InceptionNet, and EfficientNet. As we’ll see, the EfficientNet model ended up having the highest accuracy. . The ResNET model (a tweaked version of our second baseline model) . One simple approach we took to build upon our second, and more sophisticated, baseline model; this was a (slightly) finetuned version of a ResNET50 model, courtesy of Zach Mueller. . Since we’ve already explained the rough contours of the model in our previous blog post, we won’t spend much time here talking about we did the previous round. That said, it’s worth briefly explaining the ResNET architecture. . What is the ResNET architecture . This was invented by Kaiming He et al in 2015; the key innovation, at the risk of over-simplifying, was to use so-called ‘skip connections’ — connections between layers that skip over layers — that make deeper networks easier to train. (For more details, see either the original paper, or FastAI’s explanations of it. . What we did this time . As we’d mentioned in the previous blogpost, the model was implemented using the FastAI library. Although our previous version of the model employed some data augmentation, we’d confined ourselves to the basic ones that are already built into FastAI; our main experiment this time was to try doing even more data augmentation with albumentations library (by adapting another one of Mueller’s notebooks). In particular, we added transforms like hue and saturation tweaks, as well as ‘cutout’ and ‘coarse dropout’. The last two are worth emphasizing: they basically involve randomly remove rectangles from the image during training (cutout removes one large but otherwise randomly sized square, whereas coarse dropout removes many small similarly-sized rectangles). . Adding these augmentations turned out to be more effective than we’d had expected. This model had already got to 79% validation accuracy with just 1 frozen epoch and 3 epochs, even before adding these more sophisticated augmentations; after adding them, we got to 84% accuracy (with the same number of epochs). That’s a big difference, for not very much work. . . But why, you might ask, did adding these augmentations make such a big difference? We speculate that it’s because these augmentations — as with any sort of data augmentation — serve to regularize the model, preventing it from overfitting to the training data. (This, of course, doesn’t yet answer why it made such a big difference. But trying to answer that would bring us too far afield.) . Links / resources / references . Our Colab notebook for both the baseline and tweaked model can be found here. | Again, in making this notebook, we adapted two of Zach Mueller’s notebooks: https://www.kaggle.com/muellerzr/cassava-fastai-starter and https://www.kaggle.com/muellerzr/recreating-abhishek-s-tez-with-fastai | Here’s the paper that introduced ResNET to the world. | And the paper on cutout can be found here | . EfficientNet model . The second architecture we tried was EfficientNet, via this notebook. . EfficientNet architecture . EfficientNet uses the compound-scaling(balancing dimensions of the width, depth, and resolution by scaling up them with a constant ratio) method to heuristic scale-up convolution neural networks (Tan and Le, 2019) to avoid computational expensive grid-search of hyperparameter. There is a family of EfficientNet models that makes up a good combination of efficiency and accuracy on various scales. Keras API provides pre-trained EfficientNet models with different variants from B0 to B7. The input shape is different for each of these variants. . What we did . We implemented the pre-trained EfficientNetB3 model based on the input image resolution. On top of the pre-trained EfficientNetB3 model, we first did global average pooling. Then we added a dense layer with 256 neurons, with an activation function of Relu, followed by a dropout layer with a dropout rate of 0.5. Finally a dense output layer with softmax activation function with 5 neurons. The model architecture looks like this: . . We implemented Early Stopping and ReduceLROnPlateau callbacks in the model. The early stopping callback stop training once a monitored metric has stopped improving. In our case, the monitored metric is validation loss. The model training loop will check at the end of every epoch if the validation loss is no longer decreasing based on some user-customized criteria. For example, in our model, we set min_delta = 0.001, which is the minimum change in the validation loss we set to qualify as an improvement, and patience = 7, the number of epochs with no improvement after which training will be stopped. Once the model training loop found the validation loss is no longer decreasing based upon these criteria, the training will terminate. Similarly, the ReduceLROnPlateau callback reduces the learning rate when the monitored metric stopped improving. Many models would benefit from reducing the learning rate by a factor of 2-10 once learning stopped. It allows us to monitor the metric we are interested in; and if we see no improvement for patience (we set to 2) number of epochs, the learning rate will be reduced. . We used the Adam optimizer, the categorical cross-entropy with 0.3 label smoothing was used given there is some mislabelling in the dataset. We split the data 80% for training, 20% for testing. We set to train the model for 30 epochs but it stopped early at 24 epochs since the early stopping criteria has been met. Finally, the model achieved an accuracy of around 89%. . Train and validation loss and accuracy figures are shown below: . . Resources / references . Tan and Le,2019. https://arxiv.org/pdf/1905.11946.pdf . Keras’ tutorial on fine-tuning Efficient Net . Inception . The third model we tried is inception-v3 pre-trained on ImageNet. Compared to EfficientNet, it performs slightly worse on ImageNet and on the Cassava dataset. It gave a validation/testing accuracy of 84%. Inception-v3 is the third version of Google’s inception network series (googlenet/inception) and is proven to be significantly computationally efficient. . Inception v3 Architecture . The architecture of an Inception v3 network is progressively built, step-by-step, as follows: . Factorized convolutions to reduce parameters | Smaller convolutions | Asymmetric convolution | Auxiliary classifier: an auxiliary classifier is a small CNN inserted between layers during training, and the loss incurred is added to the main network loss | Grid size reduction | For the head of the model we added a dropout layer and a dense softmax layer with 5 neurons. The optimizer used was SGD and loss was CategoricalCrossEntropy with 0.2 label smoothing. The label smoothing is because there is some amount of mislabelling in the dataset. The base notebook we used also used Xception for the base model, however since there were very small differences in the validation accuracy and loss, we chose to keep Inception-v3 as our 3rd model. . We tried training with multiple epochs but there was no significant increase in val. accuracy after 8 epochs (as in the original notebook) and since every epoch took upwards of 10 minutes for us, in the interest of time we kept it at 8. The validation accuracy we got in the end was 84%. . Resources / references . Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., &amp; Rabinovich, A. (2014). Going deeper with convolutions. . Vihar Kurama. (2020, June 5). A Guide to ResNet, Inception v3, and SqueezeNet Paperspace Blog. Paperspace Blog; Paperspace Blog. . Jun Ying’s Kaggle notebook . Bergman A, Lindell D. Factorized Convolution Kernels in Image Processing. Accessed March 16, 2021. . Conclusion . We’ve seen how EfficientNet got the best accuracy of the three architectures. Although this obviously wasn’t a carefully controlled comparison — there were a thousand and one variables that we didn’t control — it’s at least suggestive, especially in light of how the best-performing teams on this Kaggle competition also used EfficientNet. . What further steps will we take? Perhaps the most obvious, in light of our discussion, is: use EffficientNet! Less obviously, but more importantly, since at least some of our experiments were performed on imbalanced validation data sets, we also want to try seeing how our models perform with more balanced ones (c.f. our teacher Bo Qing’s comments on another group’s work during office hours). We also hope to carry out more experiments with data augmentation, and of course do the almost-obligatory hyperparameter tuning. Finally, we have a bunch of snazzy model interpretability visualizations lined up, so stay tuned! .",
            "url": "https://ym-han.github.io/2040mtermblog/markdown/2020/03/15/post2.html",
            "relUrl": "/markdown/2020/03/15/post2.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ym-han.github.io/2040mtermblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ym-han.github.io/2040mtermblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ym-han.github.io/2040mtermblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}