---
toc: true
layout: post
description: Team λ's 2nd midterm project blog post
categories: [markdown]
title: Post 2 of 3
---

This is the second of the blog posts for our midterm project in Data 2040, a deep learning class at Brown University. As quick reminder, the first blog post [can be accessed here.](https://emmanuel-peters.medium.com/blog-post-1-cassava-leaf-distribution-c6c3232be9d5)

We explored 3 architectures for this prediction task: ResNET, InceptionNet, and EfficientNet. As we'll see, the EfficientNet model ended up having the highest accuracy.

# The ResNET model (a tweaked version of our second baseline model)

One simple approach we took to build upon our second, and more sophisticated, baseline model; this was a (slightly) finetuned version of a ResNET50 model, [courtesy of Zach Mueller](https://www.kaggle.com/muellerzr/cassava-fastai-starter). 

Since we've already explained the rough contours of the model in our previous blog post, we won't spend much time here talking about we did the previous round. That said, it's worth briefly explaining the ResNET architecture.

## What is the ResNET architecture

This was invented by Kaiming He et al in 2015; the key innovation, at the risk of over-simplifying, was to use so-called 'skip connections' --- connections between layers that skip over layers --- that make deeper networks easier to train. (For more details, see either [the original paper](https://arxiv.org/abs/1512.03385), or [FastAI's explanations of it](https://github.com/fastai/fastbook and http://d2l.ai/).

## What we did this time

As we'd mentioned in the previous blogpost, the model was implemented using the FastAI library. Although our previous version of the model employed some data augmentation, we'd confined ourselves to the basic ones that are already built into FastAI; our main experiment this time was to try doing even more data augmentation with albumentations library ([by adapting another one of Mueller's notebooks](https://www.kaggle.com/muellerzr/recreating-abhishek-s-tez-with-fastai
)). In particular, we added transforms like hue and saturation tweaks, as well as ['cutout'](https://arxiv.org/pdf/1708.04552.pdf) and 'coarse dropout'. The last two are worth emphasizing: they basically involve randomly remove rectangles from the image during training (cutout removes one large but otherwise randomly sized square, whereas coarse dropout removes many small similarly-sized rectangles). 

Adding these augmentations turned out to be more effective than we'd had expected. This model had already got to **79%** validation accuracy with just 1 frozen epoch and 3 epochs, even before adding these more sophisticated augmentations; after adding them, we got to **84%** accuracy (with the same number of epochs). That's a big difference, for not very much work.

![]({{ site.baseurl }}/images/second_post/loss_accs_firstmodel.png "Losses and validation accuracies for the model, before and after adding these augmentations")
![]({{ site.baseurl }}/images/second_post/resnet_loss_acc_curve_aug_model.png "Losses and validation curves for the model, after adding these augmentations")

But why, you might ask, did adding these augmentations make such a big difference? We speculate that it's because these augmentations --- as with any sort of data augmentation --- serve to regularize the model, preventing it from overfitting to the training data. (This, of course, doesn't yet answer why it made such a *big* difference. But trying to answer that would bring us too far afield.) 

## Links / resources / references

* Our Colab notebook for both the baseline and tweaked model [can be found here](https://colab.research.google.com/drive/1n_jN7X1D8kB9lCicYcYaFvzwUCVViXg5?usp=sharing). 
* Again, in making this notebook, we adapted two of Zach Mueller's notebooks: https://www.kaggle.com/muellerzr/cassava-fastai-starter and https://www.kaggle.com/muellerzr/recreating-abhishek-s-tez-with-fastai
* [Here's the paper that introduced ResNET to the world.](https://arxiv.org/abs/1512.03385)
* And the paper on cutout [can be found here](https://arxiv.org/pdf/1708.04552.pdf)

# EfficientNet model

The second architecture we tried was EfficientNet, [via this notebook](https://www.kaggle.com/bununtadiresmenmor/starter-keras-efficientnet). 

## EfficientNet architecture

EfficientNet uses the compound-scaling(balancing dimensions of the width, depth, and resolution by scaling up them with a constant ratio) method to heuristic scale-up convolution neural networks [(Tan and Le, 2019)](https://arxiv.org/pdf/1905.11946.pdf) to avoid computational expensive grid-search of hyperparameter. There is a family of EfficientNet models that makes up a good combination of efficiency and accuracy on various scales. Keras API provides pre-trained EfficientNet models with different variants from B0 to B7. The input shape is different for each of these variants.

## What we did

We implemented the pre-trained EfficientNetB3 model based on the input image resolution. On top of the pre-trained EfficientNetB3 model, we first did global average pooling. Then we added a dense layer with 256 neurons, with an activation function of Relu, followed by a dropout layer with a dropout rate of 0.5. Finally a dense output layer with softmax activation function with 5 neurons. The model architecture looks like this:

![]({{ site.baseurl }}/images/second_post/effNetArch.png "EfficientNet architecture")

We implemented Early Stopping and ReduceLROnPlateau callbacks in the model. The early stopping callback stop training once a monitored metric has stopped improving. In our case, the monitored metric is validation loss. The model training loop will check at the end of every epoch if the validation loss is no longer decreasing based on some user-customized criteria. For example, in our model, we set min_delta = 0.001, which is the minimum change in the validation loss we set to qualify as an improvement, and patience = 7, the number of epochs with no improvement after which training will be stopped. Once the model training loop found the validation loss is no longer decreasing based upon these criteria, the training will terminate. Similarly, the ReduceLROnPlateau callback reduces the learning rate when the monitored metric stopped improving. Many models would benefit from reducing the learning rate by a factor of 2-10 once learning stopped. It allows us to monitor the metric we are interested in; and if we see no improvement for patience (we set to 2)  number of epochs, the learning rate will be reduced. 

We used the Adam optimizer, the categorical cross-entropy with 0.3 label smoothing was used given there is some mislabelling in the dataset. We split the data 80% for training, 20% for testing.  We set to train the model for 30 epochs but it stopped early at 24 epochs since the early stopping criteria has been met. Finally, the model achieved an accuracy of around **89%**.

Train and validation loss and accuracy figures are shown below:

![]({{ site.baseurl }}/images/second_post/lossValEffCurves.png "Train val loss accuracy curves")

## Resources / references

Tan and Le,2019. https://arxiv.org/pdf/1905.11946.pdf

[Keras' tutorial on fine-tuning Efficient Net](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning) 

# Inception

The third model we tried is inception-v3 pre-trained on ImageNet. Compared to EfficientNet, it performs slightly worse on ImageNet and on the Cassava dataset. It gave a validation/testing accuracy of 84%. Inception-v3 is the third version of Google’s inception network series (googlenet/inception) and is proven to be significantly computationally efficient.

## Inception v3 Architecture

The architecture of an Inception v3 network is progressively built, step-by-step, as follows: 

1. Factorized convolutions to reduce parameters
2. Smaller convolutions
3. Asymmetric convolution
4. Auxiliary classifier: an auxiliary classifier is a small CNN inserted between layers during training, and the loss incurred is added to the main network loss
5. Grid size reduction

 
For the head of the model we added a dropout layer and a dense softmax layer with 5 neurons. The optimizer used was SGD and loss was CategoricalCrossEntropy with 0.2 label smoothing. The label smoothing is because there is some amount of mislabelling in the dataset. The base notebook we used also used Xception for the base model, however since there were very small differences in the validation accuracy and loss, we chose to keep Inception-v3 as our 3rd model.

We tried training with multiple epochs but there was no significant increase in val. accuracy after 8 epochs (as in the original notebook) and since every epoch took upwards of 10 minutes for us, in the interest of time we kept it at 8.


## Resources / references

[Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., & Rabinovich, A. (2014). Going deeper with convolutions.](https://arxiv.org/pdf/1409.4842.pdf) 
 
[Vihar Kurama. (2020, June 5). A Guide to ResNet, Inception v3, and SqueezeNet 
Paperspace Blog. Paperspace Blog; Paperspace Blog.](https://blog.paperspace.com/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/) 

[Jun Ying's Kaggle notebook](https://www.kaggle.com/junyingsg/end-to-end-cassava-disease-classification-in-keras)

[Bergman A, Lindell D. Factorized Convolution Kernels in Image Processing. Accessed March 16, 2021.](http://stanford.edu/class/ee367/Winter2019/bergman_report.pdf)


# Conclusion

We've seen how EfficientNet got the best accuracy of the three architectures. Although this obviously wasn't a carefully controlled comparison --- there were a thousand and one variables that we didn't control --- it's at least suggestive, especially in light of how the best-performing teams on this Kaggle competition also used EfficientNet. 

What further steps will we take? Perhaps the most obvious, in light of our discussion, is: use EffficientNet! We also hope to carry out more experiments with data augmentationm, and of course do the almost-obligatory hyperparameter tuning. We also have a bunch of snazzy model interpretability visualizations lined up, so stay tuned!
