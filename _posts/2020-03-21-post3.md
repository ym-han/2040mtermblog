---
toc: true
layout: post
description: Team Î»'s 2nd midterm project blog post
categories: [markdown]
title: Cassava leaf disease classification ensemble models (Post 3 of 3)
---
# Cassava leaf disease classification ensemble models

By Peipei Li, Emmanuel Peters, Yongming Han

This is the final blog post of our data2040 midterm group project on cassava leaf disease classification. 

Here are the links to the previous posts:

> Initial blog post
https://emmanuel-peters.medium.com/blog-post-1-cassava-leaf-distribution-c6c3232be9d5

> Midway blog post
https://ym-han.github.io/2040mtermblog/markdown/2020/03/15/post2.html

We previously tried three different models: ResNet, EfficientNet, and InceptionNet. In the final model, we decided to try to ensemble those three models in a variety of ways to see if we could make better predictions of the leaf disease classification. 

Now, given how imbalanced the data was, a natural question is: how did we construct the validation set? In particular, what was the *distribution* of data sets that we used to train and validate our models?

One way to do this, of course, is to manually construct a more balanced dataset. We didn't end up doing this, though it's at the top of the list of Things We Would Do Had We More Time. Instead, we simply used the distribution of the test set on Kaggle as the distribution of our validation set (this turned out to be more or less what you'd get with by randomly sampling the data, so it was no work at all).

![](https://i.imgur.com/kqNVABa.png Distribution of test set)

![]({{ site.baseurl }}/images/second_post/loss_accs_firstmodel.png "Losses and validation accuracies for the model, before and after adding these augmentations")

How did we get the distribution of the test set? We did by submitting a constant classifier for each of the classes. That is, for each class, we submitted a notebook that predicted that class on all of the inputs. The accuracy of that classifier will in effect be the prevalence of the class in the test data. And since Kaggle helpfully tells us how accurate submitted notebooks are, we can recover the distribution of the test data by submitting a constant classifier for each of the five classes.

> "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives."

https://en.wikipedia.org/wiki/Ensemble_learning

For our final model we tried various approaches to ensemble the models we trained for our Midway blog post. 

### Approach 1

![](https://i.imgur.com/5QMiMpI.jpg)

We started by doing a simple average of our three models (ResNet50, Inception, EfficientNet) we talked about during our midterm blogpost, this gave us an accuracy score of 80%. This rather low score is probably because our first model (ResNet50) had not been tuned properly. To combat this, there were two approaches:

1. Tune the ResNet50 and retrain it.

2. Use a weighted average with a lower weight on ResNet50
We decided on starting with 2. so we looped over a grid of weights, and improved performance on the validation set to ~86%.

The second approach we did was to try ensembling different combination of two out of three of our pre-trained models. Specifically, we tried the combination of ResNet and InceptionNet, EfficientNet and InceptionNet, and ResNet and EfficientNet. For this, all we had to change was the grid to include 0 and only print cases where two networks had non zero weights. It should be noted that on certain samplings of the validation set we had single networks performing better than any combination including them, but on average we got higher performance for combinations of two or more.

We got the highest weighted average of combination for InceptionNet and EfficientNet for an accuracy of 85.6/86%. 


### Approach 2 

![](https://i.imgur.com/3KgfbJg.jpg)

Instead of using a simple average on the weights outputted by the softmax of the different CNNs, the second approach was to use stacking. To do this we needed to make sure the input tensors to the CNNs were similar and thus we had to retrain the models and make sure they fit a singular workflow. So, we focussed on a better tuned ResNet50 and Inception (we made this choice despite the higher accuracy score of EfficientNet because we had to retune ResNet50 anyway and Inception was the most computationally efficient network out of the 3).

For stacking, we take the outputs of two different networks and use them as an input to an "ensemble" network. 

It can be argued that all ensembling is a form of stacking. The highest accuracy score we got for this was also ~84%. 


### Approach 3

While looking for documentation for InceptionNet we came across this blogpost https://ai.googleblog.com/2016/08/improving-inception-and-image.html. Which talks about a stack of InceptionNetV3 with various Residual blocks from ResNet. We used this as our third ensemble. It trained faster than Approach 2, and gave us higher validation accuracy on most random seeds and validation subsets.
Stacking resnet and inception (done by google)- ResnetInceptionV2. ~85%

#### Some plots / figures for InceptionResnetv2

<!-- Loss and accuracy curves for InceptionResnetv2-->
![](https://i.imgur.com/5dsPaKZ.png "Loss and accuracy curves for InceptionResnetv2")


```
Classification Report for InceptionResNet
              precision    recall  f1-score   support

           0       0.48      0.34      0.40       200
           1       0.73      0.75      0.74       436
           2       0.78      0.66      0.72       495
           3       0.95      0.94      0.95      2638
           4       0.59      0.74      0.66       510

    accuracy                           0.84      4279
   macro avg       0.71      0.69      0.69      4279
weighted avg       0.84      0.84      0.84      4279
```



##### Confusion matrix for InceptionResNet
![](https://i.imgur.com/dQk847d.png "Confusion matrix for InceptionResNet")

As we can see from the confuson matrix, class 3 (Cassava Mosaic Disease (CMD)) has the highest chance of been correctly classified. This is not surprising because the CMD leaf has severe shape distortion and mosaic patterns.

By contrast, our model has the most difficult time with class 0 (Cassava Bacterial Blight (CBB)). Although the Cassava Bacterial Blight type has some been documented as angular spots, brown spots with yellow borders, yellow leaves, leaves wilting, those characteristics are not prominent or salient enough as features in the rest of classes. Of course, the way that the dataset was imbalanced partially contributed to the results. 

Some pictures of the two classes mentioned above are shown here: 

![](https://i.imgur.com/Z3ppTvH.png)

![](https://i.imgur.com/6HElztu.png)


### Approach 4

We finally come to our best model where we used a weighted average on the Approach 3 stack with EfficientNetB3. 

This gave us the highest validation accuracy at ~87% with our pretrained models. So we decided to spend some time changing the hyperparameters and improving the individual CNNs. Unfortunately, in the interest of time, we had to keep the number of epochs low and thus never hit our early stopping callbacks. This was true even when patience value was set to <5. We also added label smoothing to InceptionResNetV2 and reduced level of label smoothing in EfficientNet. 

Our final Validation accuracy score was ~89%.

## Resources/Citations 

* Improving Inception and Image Classification in TensorFlow. (2016, August 31). https://ai.googleblog.com/2016/08/improving-inception-and-image.html
* Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. (2017). Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 31(1). Retrieved from https://ojs.aaai.org/index.php/AAAI/article/view/11231
* https://www.kaggle.com/junyingsg/end-to-end-cassava-disease-classification-in-keras
* https://www.kaggle.com/iamyajat/cassava-leaf-disease-inceptionresnetv2
* https://www.kaggle.com/danpotter/blind-monkey-submission-example-data2040-sp21/data?select=submission.csv
* Maxim Mikhaylov. (2017, December 13). Ensembling ConvNets using Keras  https://towardsdatascience.com/ensembling-convnets-using-keras-237d429157eb
* Adrian Rosebrock Deep Learning for Computer Vision with Python -- Practitioner Bundle


